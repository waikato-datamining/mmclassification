# MMClassification

Allows processing of images with [MMClassification](https://github.com/open-mmlab/mmclassification).

Uses PyTorch 1.9.0 on the CPU.

## Version

MMClassification github repo tag/hash:

```
v0.23.1
d2e505415040bf5329ab218bb6fe3d899f176cd5
```

and timestamp:

```
June 16th, 2022
```

## Docker

### Quick start

* Log into registry using *public* credentials:

  ```bash
  docker login -u public -p public public.aml-repo.cms.waikato.ac.nz:443 
  ```

* Pull and run image (adjust volume mappings `-v`):

  ```bash
  docker run --gpus=all --shm-size 8G \
    -v /local/dir:/container/dir \
    -it public.aml-repo.cms.waikato.ac.nz:443/open-mmlab/mmclassification:0.23.1_cpu
  ```

  **NB:** For docker versions older than 19.03 (`docker version`), use `--runtime=nvidia` instead of `--gpus=all`.

* If need be, remove all containers and images from your system:

  ```bash
  docker stop $(docker ps -a -q) && docker rm $(docker ps -a -q) && docker system prune -a
  ```

### Docker hub

The image is also available from [Docker hub](https://hub.docker.com/u/waikatodatamining):

```
waikatodatamining/mmclassification:0.23.1_cpu
```

### Build local image

* Build the image from Docker file (from within /path_to/mmclassification/0.23.1_cpu)

  ```bash
  docker build -t mmcls .
  ```
  
* Run the container

  ```bash
  docker run --gpus=all --shm-size 8G -v /local/dir:/container/dir -it mmcls
  ```
  `/local/dir:/container/dir` maps a local disk directory into a directory inside the container

### Scripts

The following scripts are available:

* `mmcls_predict_poll` - for applying a model to images (uses file-polling, calls `/mmclassification/tools/predict_poll.py`)
* `mmcls_predict_redis` - for applying a model to images (via [Redis](https://redis.io/) backend), 
  add `--net=host` to the Docker options (calls `/mmclassification/tools/predict_redis.py`)


### Usage

* Predict and output JSON files with the classes and their associated scores

  ```bash
  mmcls_predict_poll \
      --model /path_to/epoch_n.pth \
      --config /path_to/your_data_config.py \
      --prediction_in /path_to/test_imgs \
      --prediction_out /path_to/test_results
  ```
  Run with `-h` for all available options.

* Predict via Redis backend

  You need to start the docker container with the `--net=host` option if you are using the host's Redis server.

  The following command listens for images coming through on channel `images` and broadcasts
  predicted images on channel `predictions`:

  ```bash
  mmcls_predict_redis \
      --model /path_to/epoch_n.pth \
      --config /path_to/your_data_config.py \
      --redis_in images \
      --redis_out predictions
  ```
  
  Run with `-h` for all available options.


## Publish images

### Build

```bash
docker build -t open-mmlab/mmclassification:0.23.1_cpu .
```

### Inhouse registry  

* Tag

  ```bash
  docker tag \
    mmclassification:0.23.1_cpu \
    public-push.aml-repo.cms.waikato.ac.nz:443/open-mmlab/mmclassification:0.23.1_cpu
  ```
  
* Push

  ```bash
  docker push public-push.aml-repo.cms.waikato.ac.nz:443/open-mmlab/mmclassification:0.23.1_cpu
  ```
  If error "no basic auth credentials" occurs, then run (enter username/password when prompted):
  
  ```bash
  docker login public-push.aml-repo.cms.waikato.ac.nz:443
  ```

### Docker hub  

* Tag

  ```bash
  docker tag \
    mmclassification:0.23.1_cpu \
    waikatodatamining/mmclassification:0.23.1_cpu
  ```
  
* Push

  ```bash
  docker push waikatodatamining/mmclassification:0.23.1_cpu
  ```
  If error "no basic auth credentials" occurs, then run (enter username/password when prompted):
  
  ```bash
  docker login
  ``` 

## Permissions

When running the docker container as regular use, you will want to set the correct
user and group on the files generated by the container (aka the user:group launching
the container):

```bash
docker run -u $(id -u):$(id -g) -e USER=$USER ...
```

## Caching models

PyTorch downloads base models, if necessary. However, by using Docker, this means that 
models will get downloaded with each Docker image, using up unnecessary bandwidth and
slowing down the startup. To avoid this, you can map a directory on the host machine
to cache the base models for all processes (usually, there would be only one concurrent
model being trained):  

```
-v /somewhere/local/cache:/.cache
```

Or specifically for PyTorch:

```
-v /somewhere/local/cache/torch:/.cache/torch
```

**NB:** When running the container as root rather than a specific user, the internal directory will have to be
prefixed with `/root`. 
